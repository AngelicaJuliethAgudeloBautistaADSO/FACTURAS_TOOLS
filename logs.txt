
==> Audit <==
|----------------|------------------------|----------|------------------------|---------|---------------------|---------------------|
|    Command     |          Args          | Profile  |          User          | Version |     Start Time      |      End Time       |
|----------------|------------------------|----------|------------------------|---------|---------------------|---------------------|
| start          |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 14:22 -05 | 20 Feb 25 14:26 -05 |
| stop           |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 15:54 -05 | 20 Feb 25 15:54 -05 |
| start          |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 15:54 -05 | 20 Feb 25 15:55 -05 |
| update-context |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 15:55 -05 | 20 Feb 25 15:55 -05 |
| update-context |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 15:56 -05 | 20 Feb 25 15:56 -05 |
| stop           |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 15:59 -05 | 20 Feb 25 15:59 -05 |
| delete         |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 15:59 -05 | 20 Feb 25 15:59 -05 |
| start          | --apiserver-port=52584 | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 15:59 -05 | 20 Feb 25 16:00 -05 |
| stop           |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:05 -05 | 20 Feb 25 16:05 -05 |
| delete         |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:06 -05 | 20 Feb 25 16:06 -05 |
| start          |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:06 -05 | 20 Feb 25 16:06 -05 |
| stop           |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:07 -05 | 20 Feb 25 16:07 -05 |
| delete         |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:07 -05 | 20 Feb 25 16:07 -05 |
| update-context |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:20 -05 |                     |
| start          |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:21 -05 | 20 Feb 25 16:23 -05 |
| update-context |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:26 -05 | 20 Feb 25 16:26 -05 |
| dashboard      |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:31 -05 |                     |
| stop           |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:51 -05 | 20 Feb 25 16:51 -05 |
| delete         |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:51 -05 | 20 Feb 25 16:51 -05 |
| start          | --apiserver-port=3000  | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:52 -05 | 20 Feb 25 16:53 -05 |
| config         | view                   | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:58 -05 | 20 Feb 25 16:58 -05 |
| config         | view                   | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 16:59 -05 | 20 Feb 25 16:59 -05 |
| config         | view                   | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:00 -05 | 20 Feb 25 17:00 -05 |
| stop           |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:02 -05 | 20 Feb 25 17:02 -05 |
| delete         |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:02 -05 | 20 Feb 25 17:02 -05 |
| start          |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:05 -05 | 20 Feb 25 17:06 -05 |
| service        | vue-app-service        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:07 -05 |                     |
| service        | vue-app-service        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:13 -05 |                     |
| service        | vue-app-deployment     | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:14 -05 |                     |
| stop           |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:22 -05 | 20 Feb 25 17:22 -05 |
| delete         |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:23 -05 | 20 Feb 25 17:23 -05 |
| docker-env     |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:23 -05 |                     |
| start          |                        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:27 -05 | 20 Feb 25 17:28 -05 |
| service        | vue-app-service        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:29 -05 |                     |
| service        | vue-app-service        | minikube | DISFARMA\ivan.guerrero | v1.35.0 | 20 Feb 25 17:31 -05 |                     |
|----------------|------------------------|----------|------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/02/20 17:27:59
Running on machine: 68307AUXDES
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0220 17:27:59.048549   38220 out.go:345] Setting OutFile to fd 96 ...
I0220 17:27:59.049547   38220 out.go:358] Setting ErrFile to fd 100...
I0220 17:27:59.118204   38220 out.go:352] Setting JSON to false
I0220 17:27:59.133275   38220 start.go:129] hostinfo: {"hostname":"68307AUXDES","uptime":181844,"bootTime":1739908634,"procs":465,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.4890 Build 22631.4890","kernelVersion":"10.0.22631.4890 Build 22631.4890","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"8b7d5347-4aaa-4b86-bc1f-a4d4b65072a7"}
W0220 17:27:59.133275   38220 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0220 17:27:59.140434   38220 out.go:177] üòÑ  minikube v1.35.0 on Microsoft Windows 11 Pro 10.0.22631.4890 Build 22631.4890
I0220 17:27:59.142064   38220 notify.go:220] Checking for updates...
I0220 17:27:59.143177   38220 driver.go:394] Setting default libvirt URI to qemu:///system
I0220 17:27:59.143177   38220 global.go:112] Querying for installed drivers using PATH=C:\Program Files\WindowsApps\Microsoft.PowerShell_7.5.0.0_x64__8wekyb3d8bbwe;C:\xampp\composer;C:\xampp\php;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Git\cmd;C:\ProgramData\ComposerSetup\bin;C:\Users\ivan.guerrero\AppData\Roaming\nvm;C:\Program Files\nodejs;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\Microsoft SQL Server\150\Tools\Binn\;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\170\Tools\Binn\;C:\Program Files\dotnet\;C:\Program Files\Cloudflare\Cloudflare WARP\;C:\Program Files\Kubernetes\Minikube;;C:\minikube;C:\Users\ivan.guerrero\AppData\Local\Microsoft\WindowsApps;C:\Users\ivan.guerrero\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\ivan.guerrero\AppData\Roaming\Composer\vendor\bin;C:\Users\ivan.guerrero\AppData\Roaming\nvm;C:\Program Files\nodejs;C:\Users\ivan.guerrero\.dotnet\tools;
I0220 17:27:59.185934   38220 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0220 17:27:59.201747   38220 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %PATH% Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0220 17:27:59.434455   38220 docker.go:123] docker version: linux-24.0.7:Docker Desktop 4.26.1 (131620)
I0220 17:27:59.442536   38220 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0220 17:28:00.033659   38220 info.go:266] docker info: {ID:d2d1f354-6e24-48f0-baec-7d084c9847ed Containers:19 ContainersRunning:19 ContainersPaused:0 ContainersStopped:0 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:175 OomKillDisable:true NGoroutines:183 SystemTime:2025-02-20 22:27:59.972544565 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:21 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:12385796096 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0220 17:28:00.034169   38220 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0220 17:28:00.059394   38220 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %PATH% Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0220 17:28:00.059394   38220 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0220 17:28:02.107881   38220 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:Hyper-V requires Administrator privileges Reason: Fix:Right-click the PowerShell icon and select Run as Administrator to open PowerShell in elevated mode. Doc: Version:}
I0220 17:28:02.129203   38220 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %PATH% Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0220 17:28:02.129203   38220 driver.go:316] not recommending "ssh" due to default: false
I0220 17:28:02.129203   38220 driver.go:311] not recommending "hyperv" due to health: Hyper-V requires Administrator privileges
I0220 17:28:02.129203   38220 driver.go:351] Picked: docker
I0220 17:28:02.129203   38220 driver.go:352] Alternatives: [ssh]
I0220 17:28:02.129203   38220 driver.go:353] Rejects: [virtualbox vmware podman hyperv qemu2]
I0220 17:28:02.132712   38220 out.go:177] ‚ú®  Automatically selected the docker driver
I0220 17:28:02.134343   38220 start.go:297] selected driver: docker
I0220 17:28:02.134343   38220 start.go:901] validating driver "docker" against <nil>
I0220 17:28:02.134343   38220 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0220 17:28:02.155747   38220 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0220 17:28:02.721669   38220 info.go:266] docker info: {ID:d2d1f354-6e24-48f0-baec-7d084c9847ed Containers:19 ContainersRunning:19 ContainersPaused:0 ContainersStopped:0 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:177 OomKillDisable:true NGoroutines:187 SystemTime:2025-02-20 22:28:02.678515028 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:22 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:12385796096 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0220 17:28:02.722197   38220 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0220 17:28:02.771238   38220 start_flags.go:393] Using suggested 6000MB memory alloc based on sys=24264MB, container=11812MB
I0220 17:28:02.771745   38220 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0220 17:28:02.773836   38220 out.go:177] üìå  Using Docker Desktop driver with root privileges
I0220 17:28:02.775506   38220 cni.go:84] Creating CNI manager for ""
I0220 17:28:02.775506   38220 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0220 17:28:02.775506   38220 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0220 17:28:02.775506   38220 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:6000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\ivan.guerrero:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0220 17:28:02.777250   38220 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0220 17:28:02.778292   38220 cache.go:121] Beginning downloading kic base image for docker with docker
I0220 17:28:02.779354   38220 out.go:177] üöú  Pulling base image v0.0.46 ...
I0220 17:28:02.780957   38220 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0220 17:28:02.780957   38220 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0220 17:28:02.780957   38220 preload.go:146] Found local preload: C:\Users\ivan.guerrero\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0220 17:28:02.780957   38220 cache.go:56] Caching tarball of preloaded images
I0220 17:28:02.781497   38220 preload.go:172] Found C:\Users\ivan.guerrero\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0220 17:28:02.781497   38220 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0220 17:28:02.782043   38220 profile.go:143] Saving config to C:\Users\ivan.guerrero\.minikube\profiles\minikube\config.json ...
I0220 17:28:02.782043   38220 lock.go:35] WriteFile acquiring C:\Users\ivan.guerrero\.minikube\profiles\minikube\config.json: {Name:mk4d322b957100119dcd4fc877c958a17cb3ca4d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 17:28:03.018351   38220 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0220 17:28:03.018351   38220 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0220 17:28:03.018351   38220 cache.go:227] Successfully downloaded all kic artifacts
I0220 17:28:03.019431   38220 start.go:360] acquireMachinesLock for minikube: {Name:mkf6792e1112c0f463315e389995f21663380f18 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0220 17:28:03.019431   38220 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0220 17:28:03.019431   38220 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:6000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\ivan.guerrero:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0220 17:28:03.019971   38220 start.go:125] createHost starting for "" (driver="docker")
I0220 17:28:03.024878   38220 out.go:235] üî•  Creating docker container (CPUs=2, Memory=6000MB) ...
I0220 17:28:03.025417   38220 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0220 17:28:03.025417   38220 client.go:168] LocalClient.Create starting
I0220 17:28:03.025975   38220 main.go:141] libmachine: Reading certificate data from C:\Users\ivan.guerrero\.minikube\certs\ca.pem
I0220 17:28:03.026533   38220 main.go:141] libmachine: Decoding PEM data...
I0220 17:28:03.026533   38220 main.go:141] libmachine: Parsing certificate...
I0220 17:28:03.026533   38220 main.go:141] libmachine: Reading certificate data from C:\Users\ivan.guerrero\.minikube\certs\cert.pem
I0220 17:28:03.026533   38220 main.go:141] libmachine: Decoding PEM data...
I0220 17:28:03.026533   38220 main.go:141] libmachine: Parsing certificate...
I0220 17:28:03.036914   38220 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0220 17:28:03.230285   38220 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0220 17:28:03.243145   38220 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0220 17:28:03.243145   38220 cli_runner.go:164] Run: docker network inspect minikube
W0220 17:28:03.459573   38220 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0220 17:28:03.459573   38220 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0220 17:28:03.459573   38220 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0220 17:28:03.467075   38220 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0220 17:28:03.838572   38220 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc00167e480}
I0220 17:28:03.838572   38220 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0220 17:28:03.848793   38220 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0220 17:28:04.166400   38220 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0220 17:28:04.166400   38220 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0220 17:28:04.182153   38220 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0220 17:28:04.388597   38220 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0220 17:28:04.597107   38220 oci.go:103] Successfully created a docker volume minikube
I0220 17:28:04.606485   38220 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0220 17:28:05.928874   38220 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib: (1.3223753s)
I0220 17:28:05.928874   38220 oci.go:107] Successfully prepared a docker volume minikube
I0220 17:28:05.928874   38220 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0220 17:28:05.928874   38220 kic.go:194] Starting extracting preloaded images to volume ...
I0220 17:28:05.944053   38220 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\ivan.guerrero\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
I0220 17:28:18.295089   38220 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\ivan.guerrero\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (12.3509051s)
I0220 17:28:18.295089   38220 kic.go:203] duration metric: took 12.3660842s to extract preloaded images to volume ...
I0220 17:28:18.307623   38220 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0220 17:28:18.983696   38220 info.go:266] docker info: {ID:d2d1f354-6e24-48f0-baec-7d084c9847ed Containers:19 ContainersRunning:19 ContainersPaused:0 ContainersStopped:0 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:176 OomKillDisable:true NGoroutines:183 SystemTime:2025-02-20 22:28:18.938697515 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:21 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:12385796096 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0220 17:28:18.992453   38220 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0220 17:28:19.638687   38220 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=6000mb --memory-swap=6000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0220 17:28:20.555785   38220 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0220 17:28:20.946220   38220 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0220 17:28:21.275605   38220 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0220 17:28:21.643167   38220 oci.go:144] the created container "minikube" has a running status.
I0220 17:28:21.643167   38220 kic.go:225] Creating ssh key for kic: C:\Users\ivan.guerrero\.minikube\machines\minikube\id_rsa...
I0220 17:28:22.130131   38220 kic_runner.go:191] docker (temp): C:\Users\ivan.guerrero\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0220 17:28:22.553215   38220 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0220 17:28:22.868263   38220 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0220 17:28:22.868263   38220 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0220 17:28:23.137794   38220 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\ivan.guerrero\.minikube\machines\minikube\id_rsa...
I0220 17:28:23.806156   38220 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0220 17:28:24.016274   38220 machine.go:93] provisionDockerMachine start ...
I0220 17:28:24.025184   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:24.237737   38220 main.go:141] libmachine: Using SSH client type: native
I0220 17:28:24.252384   38220 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf85360] 0xf87ea0 <nil>  [] 0s} 127.0.0.1 61006 <nil> <nil>}
I0220 17:28:24.252384   38220 main.go:141] libmachine: About to run SSH command:
hostname
I0220 17:28:24.414997   38220 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0220 17:28:24.414997   38220 ubuntu.go:169] provisioning hostname "minikube"
I0220 17:28:24.427193   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:24.618487   38220 main.go:141] libmachine: Using SSH client type: native
I0220 17:28:24.618487   38220 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf85360] 0xf87ea0 <nil>  [] 0s} 127.0.0.1 61006 <nil> <nil>}
I0220 17:28:24.619023   38220 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0220 17:28:24.813728   38220 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0220 17:28:24.828358   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:25.071946   38220 main.go:141] libmachine: Using SSH client type: native
I0220 17:28:25.071946   38220 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf85360] 0xf87ea0 <nil>  [] 0s} 127.0.0.1 61006 <nil> <nil>}
I0220 17:28:25.071946   38220 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0220 17:28:25.237150   38220 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0220 17:28:25.237225   38220 ubuntu.go:175] set auth options {CertDir:C:\Users\ivan.guerrero\.minikube CaCertPath:C:\Users\ivan.guerrero\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\ivan.guerrero\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\ivan.guerrero\.minikube\machines\server.pem ServerKeyPath:C:\Users\ivan.guerrero\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\ivan.guerrero\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\ivan.guerrero\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\ivan.guerrero\.minikube}
I0220 17:28:25.237225   38220 ubuntu.go:177] setting up certificates
I0220 17:28:25.237225   38220 provision.go:84] configureAuth start
I0220 17:28:25.246035   38220 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0220 17:28:25.479188   38220 provision.go:143] copyHostCerts
I0220 17:28:25.479721   38220 exec_runner.go:144] found C:\Users\ivan.guerrero\.minikube/cert.pem, removing ...
I0220 17:28:25.479721   38220 exec_runner.go:203] rm: C:\Users\ivan.guerrero\.minikube\cert.pem
I0220 17:28:25.480258   38220 exec_runner.go:151] cp: C:\Users\ivan.guerrero\.minikube\certs\cert.pem --> C:\Users\ivan.guerrero\.minikube/cert.pem (1139 bytes)
I0220 17:28:25.481862   38220 exec_runner.go:144] found C:\Users\ivan.guerrero\.minikube/key.pem, removing ...
I0220 17:28:25.481862   38220 exec_runner.go:203] rm: C:\Users\ivan.guerrero\.minikube\key.pem
I0220 17:28:25.482404   38220 exec_runner.go:151] cp: C:\Users\ivan.guerrero\.minikube\certs\key.pem --> C:\Users\ivan.guerrero\.minikube/key.pem (1675 bytes)
I0220 17:28:25.483510   38220 exec_runner.go:144] found C:\Users\ivan.guerrero\.minikube/ca.pem, removing ...
I0220 17:28:25.483510   38220 exec_runner.go:203] rm: C:\Users\ivan.guerrero\.minikube\ca.pem
I0220 17:28:25.484054   38220 exec_runner.go:151] cp: C:\Users\ivan.guerrero\.minikube\certs\ca.pem --> C:\Users\ivan.guerrero\.minikube/ca.pem (1099 bytes)
I0220 17:28:25.484594   38220 provision.go:117] generating server cert: C:\Users\ivan.guerrero\.minikube\machines\server.pem ca-key=C:\Users\ivan.guerrero\.minikube\certs\ca.pem private-key=C:\Users\ivan.guerrero\.minikube\certs\ca-key.pem org=ivan.guerrero.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0220 17:28:25.761176   38220 provision.go:177] copyRemoteCerts
I0220 17:28:25.782765   38220 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0220 17:28:25.791478   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:26.035164   38220 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61006 SSHKeyPath:C:\Users\ivan.guerrero\.minikube\machines\minikube\id_rsa Username:docker}
I0220 17:28:26.162739   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0220 17:28:26.203431   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\machines\server.pem --> /etc/docker/server.pem (1200 bytes)
I0220 17:28:26.241697   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0220 17:28:26.277755   38220 provision.go:87] duration metric: took 1.0404333s to configureAuth
I0220 17:28:26.277755   38220 ubuntu.go:193] setting minikube options for container-runtime
I0220 17:28:26.278287   38220 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0220 17:28:26.286877   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:26.497848   38220 main.go:141] libmachine: Using SSH client type: native
I0220 17:28:26.498378   38220 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf85360] 0xf87ea0 <nil>  [] 0s} 127.0.0.1 61006 <nil> <nil>}
I0220 17:28:26.498378   38220 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0220 17:28:26.656664   38220 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0220 17:28:26.656664   38220 ubuntu.go:71] root file system type: overlay
I0220 17:28:26.656664   38220 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0220 17:28:26.667078   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:26.883899   38220 main.go:141] libmachine: Using SSH client type: native
I0220 17:28:26.884442   38220 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf85360] 0xf87ea0 <nil>  [] 0s} 127.0.0.1 61006 <nil> <nil>}
I0220 17:28:26.884442   38220 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0220 17:28:27.072155   38220 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0220 17:28:27.081489   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:27.317244   38220 main.go:141] libmachine: Using SSH client type: native
I0220 17:28:27.317785   38220 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf85360] 0xf87ea0 <nil>  [] 0s} 127.0.0.1 61006 <nil> <nil>}
I0220 17:28:27.317785   38220 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0220 17:28:28.512709   38220 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-02-20 22:28:27.059660116 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0220 17:28:28.512709   38220 machine.go:96] duration metric: took 4.4963875s to provisionDockerMachine
I0220 17:28:28.512709   38220 client.go:171] duration metric: took 25.4870216s to LocalClient.Create
I0220 17:28:28.512709   38220 start.go:167] duration metric: took 25.4870216s to libmachine.API.Create "minikube"
I0220 17:28:28.512709   38220 start.go:293] postStartSetup for "minikube" (driver="docker")
I0220 17:28:28.512709   38220 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0220 17:28:28.533006   38220 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0220 17:28:28.541680   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:28.753388   38220 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61006 SSHKeyPath:C:\Users\ivan.guerrero\.minikube\machines\minikube\id_rsa Username:docker}
I0220 17:28:28.893691   38220 ssh_runner.go:195] Run: cat /etc/os-release
I0220 17:28:28.901066   38220 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0220 17:28:28.901066   38220 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0220 17:28:28.901066   38220 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0220 17:28:28.901066   38220 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0220 17:28:28.901612   38220 filesync.go:126] Scanning C:\Users\ivan.guerrero\.minikube\addons for local assets ...
I0220 17:28:28.901612   38220 filesync.go:126] Scanning C:\Users\ivan.guerrero\.minikube\files for local assets ...
I0220 17:28:28.902157   38220 start.go:296] duration metric: took 389.4438ms for postStartSetup
I0220 17:28:28.916056   38220 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0220 17:28:29.121708   38220 profile.go:143] Saving config to C:\Users\ivan.guerrero\.minikube\profiles\minikube\config.json ...
I0220 17:28:29.142954   38220 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0220 17:28:29.151096   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:29.369939   38220 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61006 SSHKeyPath:C:\Users\ivan.guerrero\.minikube\machines\minikube\id_rsa Username:docker}
I0220 17:28:29.514036   38220 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0220 17:28:29.524084   38220 start.go:128] duration metric: took 26.5038325s to createHost
I0220 17:28:29.524084   38220 start.go:83] releasing machines lock for "minikube", held for 26.5043726s
I0220 17:28:29.532789   38220 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0220 17:28:29.741775   38220 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0220 17:28:29.752800   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:29.759907   38220 ssh_runner.go:195] Run: cat /version.json
I0220 17:28:29.770973   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:29.984379   38220 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61006 SSHKeyPath:C:\Users\ivan.guerrero\.minikube\machines\minikube\id_rsa Username:docker}
I0220 17:28:30.013856   38220 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61006 SSHKeyPath:C:\Users\ivan.guerrero\.minikube\machines\minikube\id_rsa Username:docker}
W0220 17:28:30.095168   38220 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0220 17:28:30.164048   38220 ssh_runner.go:195] Run: systemctl --version
I0220 17:28:30.197088   38220 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0220 17:28:30.234449   38220 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0220 17:28:30.257720   38220 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0220 17:28:30.277517   38220 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0220 17:28:30.337366   38220 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0220 17:28:30.337366   38220 start.go:495] detecting cgroup driver to use...
I0220 17:28:30.337366   38220 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0220 17:28:30.337366   38220 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0220 17:28:30.397018   38220 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0220 17:28:30.445811   38220 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0220 17:28:30.470410   38220 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0220 17:28:30.494747   38220 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
W0220 17:28:30.505574   38220 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0220 17:28:30.506113   38220 out.go:270] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0220 17:28:30.539746   38220 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0220 17:28:30.584797   38220 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0220 17:28:30.619035   38220 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0220 17:28:30.656190   38220 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0220 17:28:30.692195   38220 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0220 17:28:30.731973   38220 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0220 17:28:30.770588   38220 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0220 17:28:30.804095   38220 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0220 17:28:30.834395   38220 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0220 17:28:30.864749   38220 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 17:28:31.001680   38220 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0220 17:28:31.157053   38220 start.go:495] detecting cgroup driver to use...
I0220 17:28:31.157053   38220 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0220 17:28:31.182827   38220 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0220 17:28:31.209170   38220 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0220 17:28:31.235979   38220 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0220 17:28:31.268975   38220 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0220 17:28:31.327900   38220 ssh_runner.go:195] Run: which cri-dockerd
I0220 17:28:31.360761   38220 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0220 17:28:31.378251   38220 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0220 17:28:31.447059   38220 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0220 17:28:31.632538   38220 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0220 17:28:31.797669   38220 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0220 17:28:31.797669   38220 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0220 17:28:31.846795   38220 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 17:28:32.001102   38220 ssh_runner.go:195] Run: sudo systemctl restart docker
I0220 17:28:32.600703   38220 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0220 17:28:32.639523   38220 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0220 17:28:32.683129   38220 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0220 17:28:32.916266   38220 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0220 17:28:33.082126   38220 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 17:28:33.250404   38220 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0220 17:28:33.298913   38220 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0220 17:28:33.345170   38220 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 17:28:33.497638   38220 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0220 17:28:33.630286   38220 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0220 17:28:33.656799   38220 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0220 17:28:33.665645   38220 start.go:563] Will wait 60s for crictl version
I0220 17:28:33.685801   38220 ssh_runner.go:195] Run: which crictl
I0220 17:28:33.719405   38220 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0220 17:28:33.781519   38220 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0220 17:28:33.791917   38220 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0220 17:28:33.845627   38220 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0220 17:28:33.883836   38220 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0220 17:28:33.893194   38220 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0220 17:28:34.191873   38220 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0220 17:28:34.212851   38220 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0220 17:28:34.221659   38220 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0220 17:28:34.251783   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0220 17:28:34.465624   38220 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:6000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\ivan.guerrero:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0220 17:28:34.465624   38220 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0220 17:28:34.474320   38220 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0220 17:28:34.506685   38220 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0220 17:28:34.506685   38220 docker.go:619] Images already preloaded, skipping extraction
I0220 17:28:34.515967   38220 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0220 17:28:34.546374   38220 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0220 17:28:34.546374   38220 cache_images.go:84] Images are preloaded, skipping loading
I0220 17:28:34.546374   38220 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0220 17:28:34.546911   38220 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0220 17:28:34.556138   38220 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0220 17:28:34.641330   38220 cni.go:84] Creating CNI manager for ""
I0220 17:28:34.641330   38220 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0220 17:28:34.641330   38220 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0220 17:28:34.641330   38220 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0220 17:28:34.641330   38220 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0220 17:28:34.658508   38220 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0220 17:28:34.673968   38220 binaries.go:44] Found k8s binaries, skipping transfer
I0220 17:28:34.692440   38220 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0220 17:28:34.708868   38220 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0220 17:28:34.738516   38220 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0220 17:28:34.766432   38220 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0220 17:28:34.816730   38220 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0220 17:28:34.826011   38220 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0220 17:28:34.865525   38220 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 17:28:35.042057   38220 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0220 17:28:35.070982   38220 certs.go:68] Setting up C:\Users\ivan.guerrero\.minikube\profiles\minikube for IP: 192.168.49.2
I0220 17:28:35.070982   38220 certs.go:194] generating shared ca certs ...
I0220 17:28:35.071552   38220 certs.go:226] acquiring lock for ca certs: {Name:mkba993c8ae89e97505e7b120e090f5c915ee361 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 17:28:35.073823   38220 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\ivan.guerrero\.minikube\ca.key
I0220 17:28:35.074149   38220 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\ivan.guerrero\.minikube\proxy-client-ca.key
I0220 17:28:35.074149   38220 certs.go:256] generating profile certs ...
I0220 17:28:35.075240   38220 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\ivan.guerrero\.minikube\profiles\minikube\client.key
I0220 17:28:35.075240   38220 crypto.go:68] Generating cert C:\Users\ivan.guerrero\.minikube\profiles\minikube\client.crt with IP's: []
I0220 17:28:35.405753   38220 crypto.go:156] Writing cert to C:\Users\ivan.guerrero\.minikube\profiles\minikube\client.crt ...
I0220 17:28:35.405753   38220 lock.go:35] WriteFile acquiring C:\Users\ivan.guerrero\.minikube\profiles\minikube\client.crt: {Name:mke68ac7e9d7add531fe27003951eca603149004 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 17:28:35.406754   38220 crypto.go:164] Writing key to C:\Users\ivan.guerrero\.minikube\profiles\minikube\client.key ...
I0220 17:28:35.406754   38220 lock.go:35] WriteFile acquiring C:\Users\ivan.guerrero\.minikube\profiles\minikube\client.key: {Name:mk788a17bd35c61bf9c98a1a879533b986ee67f7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 17:28:35.407756   38220 certs.go:363] generating signed profile cert for "minikube": C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0220 17:28:35.407756   38220 crypto.go:68] Generating cert C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0220 17:28:35.652735   38220 crypto.go:156] Writing cert to C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0220 17:28:35.652735   38220 lock.go:35] WriteFile acquiring C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mk73d44117da8b1292e5ceddecb31c79682d79b6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 17:28:35.653738   38220 crypto.go:164] Writing key to C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0220 17:28:35.653738   38220 lock.go:35] WriteFile acquiring C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mk7d94508ff76338644f61033ade9a62e6d7850a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 17:28:35.656737   38220 certs.go:381] copying C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.crt
I0220 17:28:35.669334   38220 certs.go:385] copying C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.key
I0220 17:28:35.673541   38220 certs.go:363] generating signed profile cert for "aggregator": C:\Users\ivan.guerrero\.minikube\profiles\minikube\proxy-client.key
I0220 17:28:35.673541   38220 crypto.go:68] Generating cert C:\Users\ivan.guerrero\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0220 17:28:35.828430   38220 crypto.go:156] Writing cert to C:\Users\ivan.guerrero\.minikube\profiles\minikube\proxy-client.crt ...
I0220 17:28:35.828430   38220 lock.go:35] WriteFile acquiring C:\Users\ivan.guerrero\.minikube\profiles\minikube\proxy-client.crt: {Name:mk53e6490e00c4d8d78dca63fcf72ba719d55f5a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 17:28:35.829426   38220 crypto.go:164] Writing key to C:\Users\ivan.guerrero\.minikube\profiles\minikube\proxy-client.key ...
I0220 17:28:35.829426   38220 lock.go:35] WriteFile acquiring C:\Users\ivan.guerrero\.minikube\profiles\minikube\proxy-client.key: {Name:mk77c0835d906528762f86ad4c2b951548818e8a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 17:28:35.846702   38220 certs.go:484] found cert: C:\Users\ivan.guerrero\.minikube\certs\ca-key.pem (1679 bytes)
I0220 17:28:35.846702   38220 certs.go:484] found cert: C:\Users\ivan.guerrero\.minikube\certs\ca.pem (1099 bytes)
I0220 17:28:35.846702   38220 certs.go:484] found cert: C:\Users\ivan.guerrero\.minikube\certs\cert.pem (1139 bytes)
I0220 17:28:35.846702   38220 certs.go:484] found cert: C:\Users\ivan.guerrero\.minikube\certs\key.pem (1675 bytes)
I0220 17:28:35.848879   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0220 17:28:35.887403   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0220 17:28:35.933303   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0220 17:28:35.974358   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0220 17:28:36.013074   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0220 17:28:36.051785   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0220 17:28:36.087624   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0220 17:28:36.125446   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0220 17:28:36.168320   38220 ssh_runner.go:362] scp C:\Users\ivan.guerrero\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0220 17:28:36.208473   38220 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0220 17:28:36.267724   38220 ssh_runner.go:195] Run: openssl version
I0220 17:28:36.294984   38220 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0220 17:28:36.332371   38220 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0220 17:28:36.341416   38220 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 20 19:25 /usr/share/ca-certificates/minikubeCA.pem
I0220 17:28:36.359919   38220 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0220 17:28:36.387487   38220 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0220 17:28:36.420635   38220 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0220 17:28:36.429093   38220 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0220 17:28:36.429093   38220 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:6000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\ivan.guerrero:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0220 17:28:36.438791   38220 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0220 17:28:36.485139   38220 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0220 17:28:36.519396   38220 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0220 17:28:36.535499   38220 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0220 17:28:36.552219   38220 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0220 17:28:36.567541   38220 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0220 17:28:36.567541   38220 kubeadm.go:157] found existing configuration files:

I0220 17:28:36.584293   38220 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0220 17:28:36.599894   38220 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0220 17:28:36.620663   38220 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0220 17:28:36.661621   38220 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0220 17:28:36.682692   38220 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0220 17:28:36.708695   38220 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0220 17:28:36.748860   38220 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0220 17:28:36.767025   38220 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0220 17:28:36.790783   38220 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0220 17:28:36.835487   38220 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0220 17:28:36.855147   38220 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0220 17:28:36.877213   38220 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0220 17:28:36.895028   38220 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0220 17:28:37.007725   38220 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0220 17:28:37.015661   38220 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0220 17:28:37.118070   38220 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0220 17:28:52.487592   38220 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0220 17:28:52.487592   38220 kubeadm.go:310] [preflight] Running pre-flight checks
I0220 17:28:52.488099   38220 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0220 17:28:52.488704   38220 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0220 17:28:52.489356   38220 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0220 17:28:52.489356   38220 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0220 17:28:52.491055   38220 out.go:235]     ‚ñ™ Generating certificates and keys ...
I0220 17:28:52.492731   38220 kubeadm.go:310] [certs] Using existing ca certificate authority
I0220 17:28:52.492731   38220 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0220 17:28:52.492731   38220 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0220 17:28:52.493372   38220 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0220 17:28:52.493372   38220 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0220 17:28:52.493372   38220 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0220 17:28:52.493372   38220 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0220 17:28:52.493935   38220 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0220 17:28:52.493935   38220 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0220 17:28:52.493935   38220 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0220 17:28:52.494442   38220 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0220 17:28:52.494442   38220 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0220 17:28:52.494442   38220 kubeadm.go:310] [certs] Generating "sa" key and public key
I0220 17:28:52.494966   38220 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0220 17:28:52.494966   38220 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0220 17:28:52.494966   38220 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0220 17:28:52.495545   38220 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0220 17:28:52.495545   38220 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0220 17:28:52.495545   38220 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0220 17:28:52.496059   38220 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0220 17:28:52.496059   38220 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0220 17:28:52.497799   38220 out.go:235]     ‚ñ™ Booting up control plane ...
I0220 17:28:52.498305   38220 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0220 17:28:52.498305   38220 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0220 17:28:52.498305   38220 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0220 17:28:52.498865   38220 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0220 17:28:52.498865   38220 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0220 17:28:52.498865   38220 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0220 17:28:52.499387   38220 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0220 17:28:52.499387   38220 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0220 17:28:52.499933   38220 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.646267ms
I0220 17:28:52.500057   38220 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0220 17:28:52.500057   38220 kubeadm.go:310] [api-check] The API server is healthy after 9.008639904s
I0220 17:28:52.500563   38220 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0220 17:28:52.500563   38220 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0220 17:28:52.501156   38220 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0220 17:28:52.502237   38220 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0220 17:28:52.502237   38220 kubeadm.go:310] [bootstrap-token] Using token: x6amq8.1uo4aehciq5y6ng3
I0220 17:28:52.504641   38220 out.go:235]     ‚ñ™ Configuring RBAC rules ...
I0220 17:28:52.505153   38220 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0220 17:28:52.505153   38220 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0220 17:28:52.505697   38220 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0220 17:28:52.505697   38220 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0220 17:28:52.506246   38220 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0220 17:28:52.506246   38220 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0220 17:28:52.506778   38220 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0220 17:28:52.506778   38220 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0220 17:28:52.506778   38220 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0220 17:28:52.506778   38220 kubeadm.go:310] 
I0220 17:28:52.506778   38220 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0220 17:28:52.506778   38220 kubeadm.go:310] 
I0220 17:28:52.507337   38220 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0220 17:28:52.507337   38220 kubeadm.go:310] 
I0220 17:28:52.507337   38220 kubeadm.go:310]   mkdir -p $HOME/.kube
I0220 17:28:52.507337   38220 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0220 17:28:52.507872   38220 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0220 17:28:52.507872   38220 kubeadm.go:310] 
I0220 17:28:52.507872   38220 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0220 17:28:52.507872   38220 kubeadm.go:310] 
I0220 17:28:52.507872   38220 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0220 17:28:52.507872   38220 kubeadm.go:310] 
I0220 17:28:52.507872   38220 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0220 17:28:52.508483   38220 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0220 17:28:52.508483   38220 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0220 17:28:52.508483   38220 kubeadm.go:310] 
I0220 17:28:52.508483   38220 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0220 17:28:52.508992   38220 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0220 17:28:52.508992   38220 kubeadm.go:310] 
I0220 17:28:52.508992   38220 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token x6amq8.1uo4aehciq5y6ng3 \
I0220 17:28:52.509521   38220 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:4fe2114b0f5b747521bc13056b461f66ca8734b27d9d740ba81755b580a315c9 \
I0220 17:28:52.509585   38220 kubeadm.go:310] 	--control-plane 
I0220 17:28:52.509585   38220 kubeadm.go:310] 
I0220 17:28:52.509585   38220 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0220 17:28:52.509585   38220 kubeadm.go:310] 
I0220 17:28:52.510100   38220 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token x6amq8.1uo4aehciq5y6ng3 \
I0220 17:28:52.510100   38220 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:4fe2114b0f5b747521bc13056b461f66ca8734b27d9d740ba81755b580a315c9 
I0220 17:28:52.510100   38220 cni.go:84] Creating CNI manager for ""
I0220 17:28:52.510100   38220 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0220 17:28:52.512120   38220 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0220 17:28:52.546730   38220 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0220 17:28:52.566880   38220 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0220 17:28:52.603808   38220 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0220 17:28:52.621186   38220 ops.go:34] apiserver oom_adj: -16
I0220 17:28:52.630714   38220 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0220 17:28:52.632426   38220 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_02_20T17_28_52_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0220 17:28:52.817997   38220 kubeadm.go:1113] duration metric: took 213.6339ms to wait for elevateKubeSystemPrivileges
I0220 17:28:52.908967   38220 kubeadm.go:394] duration metric: took 16.4796935s to StartCluster
I0220 17:28:52.908967   38220 settings.go:142] acquiring lock: {Name:mk41b909bcdaf2f52911ee1e3e367b24a02bfc39 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 17:28:52.909475   38220 settings.go:150] Updating kubeconfig:  C:\Users\ivan.guerrero\.kube\config
I0220 17:28:52.911666   38220 lock.go:35] WriteFile acquiring C:\Users\ivan.guerrero\.kube\config: {Name:mkc8938c4fe575a5149650ba4d5f061306255b54 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 17:28:52.913354   38220 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0220 17:28:52.913354   38220 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0220 17:28:52.913862   38220 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0220 17:28:52.913862   38220 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0220 17:28:52.913862   38220 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0220 17:28:52.913862   38220 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0220 17:28:52.913862   38220 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0220 17:28:52.913862   38220 host.go:66] Checking if "minikube" exists ...
I0220 17:28:52.914414   38220 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0220 17:28:52.915217   38220 out.go:177] üîé  Verifying Kubernetes components...
I0220 17:28:52.954325   38220 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0220 17:28:52.954325   38220 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0220 17:28:52.957256   38220 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 17:28:53.263144   38220 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0220 17:28:53.263672   38220 host.go:66] Checking if "minikube" exists ...
I0220 17:28:53.277850   38220 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0220 17:28:53.284707   38220 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0220 17:28:53.284707   38220 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0220 17:28:53.296871   38220 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0220 17:28:53.320170   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:53.384954   38220 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0220 17:28:53.618223   38220 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0220 17:28:53.618223   38220 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0220 17:28:53.632053   38220 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61006 SSHKeyPath:C:\Users\ivan.guerrero\.minikube\machines\minikube\id_rsa Username:docker}
I0220 17:28:53.635870   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 17:28:53.821521   38220 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0220 17:28:53.896843   38220 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61006 SSHKeyPath:C:\Users\ivan.guerrero\.minikube\machines\minikube\id_rsa Username:docker}
I0220 17:28:54.046019   38220 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0220 17:28:54.227082   38220 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0220 17:28:54.231970   38220 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0220 17:28:54.242180   38220 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0220 17:28:54.515107   38220 api_server.go:52] waiting for apiserver process to appear ...
I0220 17:28:54.540397   38220 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0220 17:28:54.737567   38220 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0220 17:28:54.797467   38220 api_server.go:72] duration metric: took 1.8840916s to wait for apiserver process to appear ...
I0220 17:28:54.797467   38220 api_server.go:88] waiting for apiserver healthz status ...
I0220 17:28:54.797467   38220 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61005/healthz ...
I0220 17:28:54.822432   38220 api_server.go:279] https://127.0.0.1:61005/healthz returned 200:
ok
I0220 17:28:54.826549   38220 api_server.go:141] control plane version: v1.32.0
I0220 17:28:54.826549   38220 api_server.go:131] duration metric: took 29.0815ms to wait for apiserver health ...
I0220 17:28:54.826549   38220 system_pods.go:43] waiting for kube-system pods to appear ...
I0220 17:28:54.834371   38220 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0220 17:28:54.835983   38220 addons.go:514] duration metric: took 1.9220994s for enable addons: enabled=[storage-provisioner default-storageclass]
I0220 17:28:54.847888   38220 system_pods.go:59] 5 kube-system pods found
I0220 17:28:54.847888   38220 system_pods.go:61] "etcd-minikube" [b7497187-a9d4-4204-ae75-2d6c8cce79fc] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0220 17:28:54.847888   38220 system_pods.go:61] "kube-apiserver-minikube" [0d1fec42-d619-4f44-a8a7-e71aa815b880] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0220 17:28:54.847888   38220 system_pods.go:61] "kube-controller-manager-minikube" [bdbf4af0-d22a-4adf-88e6-bad30b020b33] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0220 17:28:54.847888   38220 system_pods.go:61] "kube-scheduler-minikube" [24ed073d-b047-4a69-9e68-475f8849acab] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0220 17:28:54.847888   38220 system_pods.go:61] "storage-provisioner" [1c67a9b7-e35b-42d9-889f-b761b82530ae] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0220 17:28:54.847888   38220 system_pods.go:74] duration metric: took 21.3381ms to wait for pod list to return data ...
I0220 17:28:54.847888   38220 kubeadm.go:582] duration metric: took 1.9345112s to wait for: map[apiserver:true system_pods:true]
I0220 17:28:54.847888   38220 node_conditions.go:102] verifying NodePressure condition ...
I0220 17:28:54.856598   38220 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0220 17:28:54.856657   38220 node_conditions.go:123] node cpu capacity is 8
I0220 17:28:54.856657   38220 node_conditions.go:105] duration metric: took 8.7697ms to run NodePressure ...
I0220 17:28:54.856657   38220 start.go:241] waiting for startup goroutines ...
I0220 17:28:54.856657   38220 start.go:246] waiting for cluster config update ...
I0220 17:28:54.856657   38220 start.go:255] writing updated cluster config ...
I0220 17:28:54.887768   38220 ssh_runner.go:195] Run: rm -f paused
I0220 17:28:55.107748   38220 start.go:600] kubectl: 1.28.2, cluster: 1.32.0 (minor skew: 4)
I0220 17:28:55.109608   38220 out.go:201] 
W0220 17:28:55.111238   38220 out.go:270] ‚ùó  C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.28.2, which may have incompatibilities with Kubernetes 1.32.0.
I0220 17:28:55.112930   38220 out.go:177]     ‚ñ™ Want kubectl v1.32.0? Try 'minikube kubectl -- get pods -A'
I0220 17:28:55.115267   38220 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 20 22:28:31 minikube dockerd[1107]: time="2025-02-20T22:28:31.682468056Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 20 22:28:31 minikube dockerd[1107]: time="2025-02-20T22:28:31.682493932Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Feb 20 22:28:31 minikube dockerd[1107]: time="2025-02-20T22:28:31.682552912Z" level=info msg="Daemon has completed initialization"
Feb 20 22:28:31 minikube dockerd[1107]: time="2025-02-20T22:28:31.786238535Z" level=info msg="API listen on /var/run/docker.sock"
Feb 20 22:28:31 minikube dockerd[1107]: time="2025-02-20T22:28:31.786288588Z" level=info msg="API listen on [::]:2376"
Feb 20 22:28:31 minikube systemd[1]: Started Docker Application Container Engine.
Feb 20 22:28:32 minikube systemd[1]: Stopping Docker Application Container Engine...
Feb 20 22:28:32 minikube dockerd[1107]: time="2025-02-20T22:28:32.021607422Z" level=info msg="Processing signal 'terminated'"
Feb 20 22:28:32 minikube dockerd[1107]: time="2025-02-20T22:28:32.024036653Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Feb 20 22:28:32 minikube dockerd[1107]: time="2025-02-20T22:28:32.025827622Z" level=info msg="Daemon shutdown complete"
Feb 20 22:28:32 minikube dockerd[1107]: time="2025-02-20T22:28:32.025982082Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Feb 20 22:28:32 minikube systemd[1]: docker.service: Deactivated successfully.
Feb 20 22:28:32 minikube systemd[1]: Stopped Docker Application Container Engine.
Feb 20 22:28:32 minikube systemd[1]: Starting Docker Application Container Engine...
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.091874534Z" level=info msg="Starting up"
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.094879788Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.140966222Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.173839210Z" level=info msg="Loading containers: start."
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.401798079Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.487732024Z" level=info msg="Loading containers: done."
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.505704070Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.505775971Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.505787704Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.505797610Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.505823394Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.505882208Z" level=info msg="Daemon has completed initialization"
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.580523145Z" level=info msg="API listen on /var/run/docker.sock"
Feb 20 22:28:32 minikube dockerd[1381]: time="2025-02-20T22:28:32.580546264Z" level=info msg="API listen on [::]:2376"
Feb 20 22:28:32 minikube systemd[1]: Started Docker Application Container Engine.
Feb 20 22:28:33 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Feb 20 22:28:33 minikube cri-dockerd[1661]: time="2025-02-20T22:28:33Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Feb 20 22:28:33 minikube cri-dockerd[1661]: time="2025-02-20T22:28:33Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Feb 20 22:28:33 minikube cri-dockerd[1661]: time="2025-02-20T22:28:33Z" level=info msg="Start docker client with request timeout 0s"
Feb 20 22:28:33 minikube cri-dockerd[1661]: time="2025-02-20T22:28:33Z" level=info msg="Hairpin mode is set to hairpin-veth"
Feb 20 22:28:33 minikube cri-dockerd[1661]: time="2025-02-20T22:28:33Z" level=info msg="Loaded network plugin cni"
Feb 20 22:28:33 minikube cri-dockerd[1661]: time="2025-02-20T22:28:33Z" level=info msg="Docker cri networking managed by network plugin cni"
Feb 20 22:28:33 minikube cri-dockerd[1661]: time="2025-02-20T22:28:33Z" level=info msg="Setting cgroupDriver cgroupfs"
Feb 20 22:28:33 minikube cri-dockerd[1661]: time="2025-02-20T22:28:33Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 20 22:28:33 minikube cri-dockerd[1661]: time="2025-02-20T22:28:33Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 20 22:28:33 minikube cri-dockerd[1661]: time="2025-02-20T22:28:33Z" level=info msg="Start cri-dockerd grpc backend"
Feb 20 22:28:33 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 20 22:28:43 minikube cri-dockerd[1661]: time="2025-02-20T22:28:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0d83ec17952a8f8c936a9d9f544ae09dbec332a64aa17e4f5613155bc8540ee5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 20 22:28:43 minikube cri-dockerd[1661]: time="2025-02-20T22:28:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/efc986c7d41c5e986ee5565d7922172c978d549f3564031c72626af02c3637e5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 20 22:28:43 minikube cri-dockerd[1661]: time="2025-02-20T22:28:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4c96d5d6c207839eb25c50c1ba8222d3993bbbd7ce3c49977b2f85c0dd77760b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 20 22:28:43 minikube cri-dockerd[1661]: time="2025-02-20T22:28:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a602e727d044011744c7ee4d26e0fb93e5132ac2defb62c9aa9b31c5e319a9c7/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 20 22:28:57 minikube cri-dockerd[1661]: time="2025-02-20T22:28:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/542963be4f558cd107108376faf5dc5563b7a5fe401eee1d1fb27219b34dbadc/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 20 22:28:58 minikube cri-dockerd[1661]: time="2025-02-20T22:28:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2d2d4ae805bdd7e5798ec2cbdcf6578827a0d0ecf75125ec56553e68db8c9c74/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 20 22:28:58 minikube dockerd[1381]: time="2025-02-20T22:28:58.430639137Z" level=info msg="ignoring event" container=6a1bc09ebaa065f4348742a3bdaa4145391e65ccc360eb8ffe3f80cac3e723dd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 20 22:28:58 minikube cri-dockerd[1661]: time="2025-02-20T22:28:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/84ebb4514d620bc40da718813b874169887f567dc5c61d5435919a8149c93ac6/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 20 22:28:59 minikube dockerd[1381]: time="2025-02-20T22:28:59.432703898Z" level=info msg="ignoring event" container=037a37e61c6481e1f368f362be27d0b87eaa994df71fcaaaf2795ca441cb62ee module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 20 22:29:02 minikube cri-dockerd[1661]: time="2025-02-20T22:29:02Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 20 22:29:14 minikube cri-dockerd[1661]: time="2025-02-20T22:29:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ba20c909c2ff84bde69a602aeea3c5357e28b52c353f92d32b4b28b2b3c89d85/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 20 22:29:16 minikube dockerd[1381]: time="2025-02-20T22:29:16.135126730Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 20 22:29:16 minikube dockerd[1381]: time="2025-02-20T22:29:16.135229145Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 20 22:29:33 minikube dockerd[1381]: time="2025-02-20T22:29:33.452299977Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 20 22:29:33 minikube dockerd[1381]: time="2025-02-20T22:29:33.452411206Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 20 22:30:02 minikube dockerd[1381]: time="2025-02-20T22:30:02.503864841Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 20 22:30:02 minikube dockerd[1381]: time="2025-02-20T22:30:02.503995361Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 20 22:30:54 minikube dockerd[1381]: time="2025-02-20T22:30:54.537646942Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 20 22:30:54 minikube dockerd[1381]: time="2025-02-20T22:30:54.537739407Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
ae67b5ba3865b       6e38f40d628db       2 minutes ago       Running             storage-provisioner       2                   542963be4f558       storage-provisioner
037a37e61c648       6e38f40d628db       3 minutes ago       Exited              storage-provisioner       1                   542963be4f558       storage-provisioner
554a27fb3c7fd       c69fa2e9cbf5f       3 minutes ago       Running             coredns                   0                   84ebb4514d620       coredns-668d6bf9bc-74mbd
939462592af12       040f9f8aac8cd       3 minutes ago       Running             kube-proxy                0                   2d2d4ae805bdd       kube-proxy-xnngs
14c46bfdefda7       a9e7e6b294baf       3 minutes ago       Running             etcd                      0                   a602e727d0440       etcd-minikube
7b6165fc79d07       8cab3d2a8bd0f       3 minutes ago       Running             kube-controller-manager   0                   4c96d5d6c2078       kube-controller-manager-minikube
891fcf41bcd94       a389e107f4ff1       3 minutes ago       Running             kube-scheduler            0                   efc986c7d41c5       kube-scheduler-minikube
a80e976643df7       c2e17b8d0f4a3       3 minutes ago       Running             kube-apiserver            0                   0d83ec17952a8       kube-apiserver-minikube


==> coredns [554a27fb3c7f] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:60323 - 58861 "HINFO IN 4478606706460183958.3183482419770465188. udp 57 false 512" NXDOMAIN qr,rd,ra 57 1.109922164s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_02_20T17_28_52_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 20 Feb 2025 22:28:48 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 20 Feb 2025 22:31:56 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 20 Feb 2025 22:29:02 +0000   Thu, 20 Feb 2025 22:28:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 20 Feb 2025 22:29:02 +0000   Thu, 20 Feb 2025 22:28:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 20 Feb 2025 22:29:02 +0000   Thu, 20 Feb 2025 22:28:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 20 Feb 2025 22:29:02 +0000   Thu, 20 Feb 2025 22:28:49 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12095504Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12095504Ki
  pods:               110
System Info:
  Machine ID:                 889eee5376b740d683d86dae046bf974
  System UUID:                889eee5376b740d683d86dae046bf974
  Boot ID:                    03f39c0c-2509-4888-b842-b3cf1119f49a
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  default                     vue-app-deployment-5f896bf957-d58jm    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m46s
  kube-system                 coredns-668d6bf9bc-74mbd               100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     3m2s
  kube-system                 etcd-minikube                          100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         3m7s
  kube-system                 kube-apiserver-minikube                250m (3%)     0 (0%)      0 (0%)           0 (0%)         3m7s
  kube-system                 kube-controller-manager-minikube       200m (2%)     0 (0%)      0 (0%)           0 (0%)         3m10s
  kube-system                 kube-proxy-xnngs                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m2s
  kube-system                 kube-scheduler-minikube                100m (1%)     0 (0%)      0 (0%)           0 (0%)         3m7s
  kube-system                 storage-provisioner                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m5s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                    From             Message
  ----     ------                             ----                   ----             -------
  Normal   Starting                           3m                     kube-proxy       
  Normal   NodeHasSufficientMemory            3m17s (x8 over 3m17s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              3m17s (x8 over 3m17s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               3m17s (x7 over 3m17s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            3m17s                  kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                           3m8s                   kubelet          Starting kubelet.
  Warning  CgroupV1                           3m8s                   kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Warning  PossibleMemoryBackedVolumesOnDisk  3m8s                   kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   NodeAllocatableEnforced            3m7s                   kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            3m7s                   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              3m7s                   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               3m7s                   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     3m3s                   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.006103] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.003194] FS-Cache: O-cookie d=000000006c62e733{9P.session} n=00000000cbd43116
[  +0.000893] FS-Cache: O-key=[10] '34323934393337363934'
[  +0.000629] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000585] FS-Cache: N-cookie d=000000006c62e733{9P.session} n=0000000027d8aeff
[  +0.003957] FS-Cache: N-key=[10] '34323934393337363934'
[Feb19 15:45] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.049862] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/America/Bogota not found. Is the tzdata package installed?
[  +0.984537] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.038099] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.005399] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.007621] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.034087] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.565064] WSL (2 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.046003] WSL (1 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00
[  +0.024182] WSL (1 - init(docker-desktop-data)) ERROR: ConfigMountFsTab:2594: Processing fstab with mount -a failed.
[  +0.002092] WSL (1 - init(docker-desktop-data)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.024221] WSL (3 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.009862] WSL (1 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00
[  +0.020736] WSL (4 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.009033] WSL (1 - init(docker-desktop-data)) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00
[  +0.023367] WSL (1 - init(docker-desktop-data)) WARNING: /usr/share/zoneinfo/America/Bogota not found. Is the tzdata package installed?
[  +0.293203] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.005427] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002571] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.008543] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.004192] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.677195] WSL (153) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +2.144327] netlink: 'init': attribute type 4 has an invalid length.
[  +1.558501] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.
[Feb20 05:39] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/America/Bogota not found. Is the tzdata package installed?
[  +0.000071] WSL (1 - init(docker-desktop-data)) WARNING: /usr/share/zoneinfo/America/Bogota not found. Is the tzdata package installed?
[Feb20 14:33] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.005043] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +4.517585] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.001121] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Feb20 19:25] tmpfs: Unknown parameter 'noswap'
[ +16.467283] hrtimer: interrupt took 2059730 ns
[  +1.289217] tmpfs: Unknown parameter 'noswap'
[Feb20 20:54] tmpfs: Unknown parameter 'noswap'
[Feb20 21:00] tmpfs: Unknown parameter 'noswap'
[  +7.191836] tmpfs: Unknown parameter 'noswap'
[Feb20 21:06] tmpfs: Unknown parameter 'noswap'
[  +6.889150] tmpfs: Unknown parameter 'noswap'
[Feb20 21:14] WSL (153) ERROR: CheckConnection: getaddrinfo() failed: -5
[Feb20 21:23] tmpfs: Unknown parameter 'noswap'
[  +9.719425] tmpfs: Unknown parameter 'noswap'
[Feb20 21:25] WSL (153) ERROR: CheckConnection: getaddrinfo() failed: -5
[Feb20 21:53] tmpfs: Unknown parameter 'noswap'
[  +9.339922] tmpfs: Unknown parameter 'noswap'
[Feb20 21:55] WSL (153) ERROR: CheckConnection: getaddrinfo() failed: -5
[Feb20 21:58] WSL (153) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +4.973111] WSL (153) ERROR: CheckConnection: getaddrinfo() failed: -5
[ +13.033184] WSL (153) ERROR: CheckConnection: getaddrinfo() failed: -5
[Feb20 22:06] tmpfs: Unknown parameter 'noswap'
[  +9.583507] tmpfs: Unknown parameter 'noswap'
[Feb20 22:26] WSL (153) ERROR: CheckConnection: getaddrinfo() failed: -5
[ +13.271544] WSL (153) ERROR: CheckConnection: getaddrinfo() failed: -5
[Feb20 22:28] tmpfs: Unknown parameter 'noswap'
[  +9.949600] tmpfs: Unknown parameter 'noswap'


==> etcd [14c46bfdefda] <==
{"level":"warn","ts":"2025-02-20T22:28:44.886151Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-02-20T22:28:44.886382Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-02-20T22:28:44.886782Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-02-20T22:28:44.886815Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-20T22:28:44.886883Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-20T22:28:44.888004Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-02-20T22:28:44.888560Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-02-20T22:28:44.901555Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"11.427581ms"}
{"level":"info","ts":"2025-02-20T22:28:44.978936Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-02-20T22:28:44.979143Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-02-20T22:28:44.979217Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-02-20T22:28:44.979238Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-02-20T22:28:44.979249Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-02-20T22:28:44.979306Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-02-20T22:28:45.026103Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-02-20T22:28:45.079141Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-02-20T22:28:45.083375Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-02-20T22:28:45.088709Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-02-20T22:28:45.089568Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-20T22:28:45.089986Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-02-20T22:28:45.090060Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-20T22:28:45.090431Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-20T22:28:45.089736Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-20T22:28:45.092711Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-02-20T22:28:45.093193Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-20T22:28:45.093309Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-02-20T22:28:45.093331Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-02-20T22:28:45.093415Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-02-20T22:28:45.093492Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-02-20T22:28:45.093519Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-02-20T22:28:45.093534Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-02-20T22:28:45.093581Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-02-20T22:28:45.094252Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-20T22:28:45.094521Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-20T22:28:45.094560Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-20T22:28:45.094829Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-02-20T22:28:45.094908Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-02-20T22:28:45.096895Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-02-20T22:28:45.096949Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-20T22:28:45.096994Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-20T22:28:45.097006Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-20T22:28:45.097977Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-02-20T22:28:45.098371Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-02-20T22:28:45.098198Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-20T22:28:45.098571Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-20T22:28:45.103919Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-20T22:28:45.104419Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-02-20T22:28:45.104486Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-20T22:28:45.104414Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-02-20T22:28:45.104533Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}


==> kernel <==
 22:31:59 up 1 day,  6:47,  0 users,  load average: 0.38, 0.65, 0.66
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [a80e976643df] <==
I0220 22:28:48.484499       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0220 22:28:48.484730       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0220 22:28:48.484776       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0220 22:28:48.502091       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0220 22:28:48.502417       1 local_available_controller.go:156] Starting LocalAvailability controller
I0220 22:28:48.502446       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0220 22:28:48.502822       1 controller.go:142] Starting OpenAPI controller
I0220 22:28:48.502875       1 controller.go:90] Starting OpenAPI V3 controller
I0220 22:28:48.502831       1 naming_controller.go:294] Starting NamingConditionController
I0220 22:28:48.502931       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0220 22:28:48.502957       1 establishing_controller.go:81] Starting EstablishingController
I0220 22:28:48.502979       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0220 22:28:48.503156       1 crd_finalizer.go:269] Starting CRDFinalizer
I0220 22:28:48.503343       1 controller.go:78] Starting OpenAPI AggregationController
I0220 22:28:48.503535       1 aggregator.go:169] waiting for initial CRD sync...
I0220 22:28:48.503540       1 controller.go:119] Starting legacy_token_tracking_controller
I0220 22:28:48.503596       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0220 22:28:48.503562       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0220 22:28:48.503618       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0220 22:28:48.503638       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0220 22:28:48.503643       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0220 22:28:48.548633       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0220 22:28:48.548713       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0220 22:28:48.548724       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0220 22:28:48.548757       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0220 22:28:48.780801       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0220 22:28:48.804489       1 cache.go:39] Caches are synced for LocalAvailability controller
I0220 22:28:48.804631       1 shared_informer.go:320] Caches are synced for configmaps
I0220 22:28:48.804982       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0220 22:28:48.805020       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0220 22:28:48.805028       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0220 22:28:48.805330       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0220 22:28:48.805368       1 aggregator.go:171] initial CRD sync complete...
I0220 22:28:48.805453       1 autoregister_controller.go:144] Starting autoregister controller
I0220 22:28:48.805473       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0220 22:28:48.805499       1 cache.go:39] Caches are synced for autoregister controller
I0220 22:28:48.806436       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0220 22:28:48.806459       1 policy_source.go:240] refreshing policies
I0220 22:28:48.878973       1 shared_informer.go:320] Caches are synced for node_authorizer
I0220 22:28:48.880167       1 controller.go:615] quota admission added evaluator for: namespaces
I0220 22:28:48.884568       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0220 22:28:48.884832       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
E0220 22:28:48.908352       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0220 22:28:49.124376       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0220 22:28:49.515550       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0220 22:28:49.529932       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0220 22:28:49.530001       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0220 22:28:50.704996       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0220 22:28:50.801969       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0220 22:28:50.898696       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0220 22:28:50.915436       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0220 22:28:50.917433       1 controller.go:615] quota admission added evaluator for: endpoints
I0220 22:28:50.927514       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0220 22:28:51.580669       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0220 22:28:51.827353       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0220 22:28:51.879255       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0220 22:28:51.893837       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0220 22:28:56.834856       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0220 22:28:57.037581       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0220 22:29:23.073580       1 alloc.go:330] "allocated clusterIPs" service="default/vue-app-service" clusterIPs={"IPv4":"10.102.54.230"}


==> kube-controller-manager [7b6165fc79d0] <==
I0220 22:28:56.085418       1 shared_informer.go:320] Caches are synced for TTL after finished
I0220 22:28:56.085751       1 shared_informer.go:320] Caches are synced for ephemeral
I0220 22:28:56.086379       1 shared_informer.go:320] Caches are synced for crt configmap
I0220 22:28:56.087184       1 shared_informer.go:320] Caches are synced for deployment
I0220 22:28:56.087937       1 shared_informer.go:320] Caches are synced for namespace
I0220 22:28:56.088835       1 shared_informer.go:320] Caches are synced for endpoint
I0220 22:28:56.090571       1 shared_informer.go:320] Caches are synced for cronjob
I0220 22:28:56.090606       1 shared_informer.go:320] Caches are synced for garbage collector
I0220 22:28:56.090672       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0220 22:28:56.090687       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0220 22:28:56.090724       1 shared_informer.go:320] Caches are synced for resource quota
I0220 22:28:56.099042       1 shared_informer.go:320] Caches are synced for ReplicationController
I0220 22:28:56.103748       1 shared_informer.go:320] Caches are synced for stateful set
I0220 22:28:56.110280       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0220 22:28:56.115628       1 shared_informer.go:320] Caches are synced for TTL
I0220 22:28:56.118497       1 shared_informer.go:320] Caches are synced for daemon sets
I0220 22:28:56.126218       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0220 22:28:56.126354       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0220 22:28:56.126276       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0220 22:28:56.126558       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0220 22:28:56.127460       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0220 22:28:56.130032       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0220 22:28:56.131380       1 shared_informer.go:320] Caches are synced for attach detach
I0220 22:28:56.132715       1 shared_informer.go:320] Caches are synced for resource quota
I0220 22:28:56.178405       1 shared_informer.go:320] Caches are synced for expand
I0220 22:28:56.178418       1 shared_informer.go:320] Caches are synced for persistent volume
I0220 22:28:56.178476       1 shared_informer.go:320] Caches are synced for garbage collector
I0220 22:28:56.178458       1 shared_informer.go:320] Caches are synced for node
I0220 22:28:56.178577       1 shared_informer.go:320] Caches are synced for service account
I0220 22:28:56.178601       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0220 22:28:56.178583       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0220 22:28:56.178798       1 shared_informer.go:320] Caches are synced for disruption
I0220 22:28:56.178862       1 shared_informer.go:320] Caches are synced for PVC protection
I0220 22:28:56.178907       1 shared_informer.go:320] Caches are synced for HPA
I0220 22:28:56.178992       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0220 22:28:56.179012       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0220 22:28:56.179026       1 shared_informer.go:320] Caches are synced for cidrallocator
I0220 22:28:56.194395       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0220 22:28:56.194446       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 22:28:56.194629       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 22:28:56.993264       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 22:28:57.336438       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="492.87496ms"
I0220 22:28:57.483062       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="145.749823ms"
I0220 22:28:57.483379       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="241.679¬µs"
I0220 22:28:57.483455       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="38.508¬µs"
I0220 22:28:59.936213       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="122.286¬µs"
I0220 22:29:02.858656       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 22:29:03.167549       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="15.976154ms"
I0220 22:29:03.167766       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="83.65¬µs"
I0220 22:29:13.682070       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/vue-app-deployment-5f896bf957" duration="53.294808ms"
I0220 22:29:13.716330       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/vue-app-deployment-5f896bf957" duration="33.838994ms"
I0220 22:29:13.716617       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/vue-app-deployment-5f896bf957" duration="85.555¬µs"
I0220 22:29:16.571475       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/vue-app-deployment-5f896bf957" duration="70.276¬µs"
I0220 22:29:32.111373       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/vue-app-deployment-5f896bf957" duration="263.35¬µs"
I0220 22:29:47.103283       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/vue-app-deployment-5f896bf957" duration="96.638¬µs"
I0220 22:30:01.109342       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/vue-app-deployment-5f896bf957" duration="67.497¬µs"
I0220 22:30:13.103426       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/vue-app-deployment-5f896bf957" duration="101.43¬µs"
I0220 22:30:25.103325       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/vue-app-deployment-5f896bf957" duration="69.566¬µs"
I0220 22:31:08.102546       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/vue-app-deployment-5f896bf957" duration="141.121¬µs"
I0220 22:31:19.098617       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/vue-app-deployment-5f896bf957" duration="60.909¬µs"


==> kube-proxy [939462592af1] <==
I0220 22:28:58.630900       1 server_linux.go:66] "Using iptables proxy"
I0220 22:28:59.096902       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0220 22:28:59.097207       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0220 22:28:59.193185       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0220 22:28:59.193395       1 server_linux.go:170] "Using iptables Proxier"
I0220 22:28:59.199946       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0220 22:28:59.278552       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0220 22:28:59.296767       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0220 22:28:59.296994       1 server.go:497] "Version info" version="v1.32.0"
I0220 22:28:59.297014       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0220 22:28:59.313296       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0220 22:28:59.329321       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0220 22:28:59.330998       1 config.go:199] "Starting service config controller"
I0220 22:28:59.331054       1 config.go:105] "Starting endpoint slice config controller"
I0220 22:28:59.331079       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0220 22:28:59.331083       1 shared_informer.go:313] Waiting for caches to sync for service config
I0220 22:28:59.331714       1 config.go:329] "Starting node config controller"
I0220 22:28:59.331735       1 shared_informer.go:313] Waiting for caches to sync for node config
I0220 22:28:59.432216       1 shared_informer.go:320] Caches are synced for service config
I0220 22:28:59.432410       1 shared_informer.go:320] Caches are synced for node config
I0220 22:28:59.432498       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [891fcf41bcd9] <==
I0220 22:28:46.134193       1 serving.go:386] Generated self-signed cert in-memory
W0220 22:28:48.601195       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0220 22:28:48.601405       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0220 22:28:48.601464       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0220 22:28:48.601505       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0220 22:28:48.882683       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0220 22:28:48.882792       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0220 22:28:48.887501       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0220 22:28:48.887677       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0220 22:28:48.890523       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0220 22:28:48.890741       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0220 22:28:48.896394       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0220 22:28:48.896538       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0220 22:28:48.896719       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0220 22:28:48.896774       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0220 22:28:48.896891       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0220 22:28:48.896926       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0220 22:28:48.897022       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0220 22:28:48.897047       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0220 22:28:48.979835       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0220 22:28:48.979835       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0220 22:28:48.979933       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0220 22:28:48.979961       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0220 22:28:48.979835       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0220 22:28:48.980060       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0220 22:28:48.980068       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0220 22:28:48.980108       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0220 22:28:48.979931       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0220 22:28:48.980170       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0220 22:28:48.980459       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0220 22:28:48.980483       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0220 22:28:48.980523       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0220 22:28:48.980256       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0220 22:28:48.980615       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0220 22:28:48.980279       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0220 22:28:48.980473       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0220 22:28:48.980676       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0220 22:28:48.980689       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0220 22:28:48.980327       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0220 22:28:48.980752       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0220 22:28:48.981140       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0220 22:28:48.981201       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0220 22:28:48.981222       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0220 22:28:49.788566       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0220 22:28:49.788681       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0220 22:28:49.867722       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0220 22:28:49.867856       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0220 22:28:49.898897       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0220 22:28:49.899030       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0220 22:28:49.898929       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0220 22:28:49.899150       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0220 22:28:49.951596       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0220 22:28:49.951701       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0220 22:28:50.006404       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0220 22:28:50.006479       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0220 22:28:50.206392       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0220 22:28:50.206477       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0220 22:28:50.318112       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0220 22:28:50.318250       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
I0220 22:28:52.588606       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Feb 20 22:28:53 minikube kubelet[2573]: I0220 22:28:53.290018    2573 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Feb 20 22:28:53 minikube kubelet[2573]: E0220 22:28:53.380728    2573 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Feb 20 22:28:53 minikube kubelet[2573]: I0220 22:28:53.399046    2573 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=4.39879509 podStartE2EDuration="4.39879509s" podCreationTimestamp="2025-02-20 22:28:49 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-20 22:28:53.300052907 +0000 UTC m=+1.560149107" watchObservedRunningTime="2025-02-20 22:28:53.39879509 +0000 UTC m=+1.658891288"
Feb 20 22:28:53 minikube kubelet[2573]: I0220 22:28:53.579713    2573 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.579683964 podStartE2EDuration="1.579683964s" podCreationTimestamp="2025-02-20 22:28:52 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-20 22:28:53.401420352 +0000 UTC m=+1.661516548" watchObservedRunningTime="2025-02-20 22:28:53.579683964 +0000 UTC m=+1.839780158"
Feb 20 22:28:53 minikube kubelet[2573]: I0220 22:28:53.579863    2573 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.579851688 podStartE2EDuration="1.579851688s" podCreationTimestamp="2025-02-20 22:28:52 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-20 22:28:53.579002468 +0000 UTC m=+1.839098666" watchObservedRunningTime="2025-02-20 22:28:53.579851688 +0000 UTC m=+1.839947886"
Feb 20 22:28:53 minikube kubelet[2573]: I0220 22:28:53.679540    2573 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.679506558 podStartE2EDuration="1.679506558s" podCreationTimestamp="2025-02-20 22:28:52 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-20 22:28:53.615272337 +0000 UTC m=+1.875368537" watchObservedRunningTime="2025-02-20 22:28:53.679506558 +0000 UTC m=+1.939602753"
Feb 20 22:28:56 minikube kubelet[2573]: I0220 22:28:56.215220    2573 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/1c67a9b7-e35b-42d9-889f-b761b82530ae-tmp\") pod \"storage-provisioner\" (UID: \"1c67a9b7-e35b-42d9-889f-b761b82530ae\") " pod="kube-system/storage-provisioner"
Feb 20 22:28:56 minikube kubelet[2573]: I0220 22:28:56.215337    2573 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6nxg8\" (UniqueName: \"kubernetes.io/projected/1c67a9b7-e35b-42d9-889f-b761b82530ae-kube-api-access-6nxg8\") pod \"storage-provisioner\" (UID: \"1c67a9b7-e35b-42d9-889f-b761b82530ae\") " pod="kube-system/storage-provisioner"
Feb 20 22:28:56 minikube kubelet[2573]: E0220 22:28:56.325632    2573 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Feb 20 22:28:56 minikube kubelet[2573]: E0220 22:28:56.325702    2573 projected.go:194] Error preparing data for projected volume kube-api-access-6nxg8 for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Feb 20 22:28:56 minikube kubelet[2573]: E0220 22:28:56.325960    2573 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/1c67a9b7-e35b-42d9-889f-b761b82530ae-kube-api-access-6nxg8 podName:1c67a9b7-e35b-42d9-889f-b761b82530ae nodeName:}" failed. No retries permitted until 2025-02-20 22:28:56.825811351 +0000 UTC m=+5.085907545 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-6nxg8" (UniqueName: "kubernetes.io/projected/1c67a9b7-e35b-42d9-889f-b761b82530ae-kube-api-access-6nxg8") pod "storage-provisioner" (UID: "1c67a9b7-e35b-42d9-889f-b761b82530ae") : configmap "kube-root-ca.crt" not found
Feb 20 22:28:57 minikube kubelet[2573]: I0220 22:28:57.224454    2573 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/e4ac0f6b-24e5-4f16-9860-46fdec1ddfcd-xtables-lock\") pod \"kube-proxy-xnngs\" (UID: \"e4ac0f6b-24e5-4f16-9860-46fdec1ddfcd\") " pod="kube-system/kube-proxy-xnngs"
Feb 20 22:28:57 minikube kubelet[2573]: I0220 22:28:57.224571    2573 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/e4ac0f6b-24e5-4f16-9860-46fdec1ddfcd-lib-modules\") pod \"kube-proxy-xnngs\" (UID: \"e4ac0f6b-24e5-4f16-9860-46fdec1ddfcd\") " pod="kube-system/kube-proxy-xnngs"
Feb 20 22:28:57 minikube kubelet[2573]: I0220 22:28:57.224612    2573 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/e4ac0f6b-24e5-4f16-9860-46fdec1ddfcd-kube-proxy\") pod \"kube-proxy-xnngs\" (UID: \"e4ac0f6b-24e5-4f16-9860-46fdec1ddfcd\") " pod="kube-system/kube-proxy-xnngs"
Feb 20 22:28:57 minikube kubelet[2573]: I0220 22:28:57.224644    2573 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vz4tj\" (UniqueName: \"kubernetes.io/projected/e4ac0f6b-24e5-4f16-9860-46fdec1ddfcd-kube-api-access-vz4tj\") pod \"kube-proxy-xnngs\" (UID: \"e4ac0f6b-24e5-4f16-9860-46fdec1ddfcd\") " pod="kube-system/kube-proxy-xnngs"
Feb 20 22:28:57 minikube kubelet[2573]: I0220 22:28:57.478857    2573 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/56ea022f-06c4-4c75-95f4-129e376ab73c-config-volume\") pod \"coredns-668d6bf9bc-74mbd\" (UID: \"56ea022f-06c4-4c75-95f4-129e376ab73c\") " pod="kube-system/coredns-668d6bf9bc-74mbd"
Feb 20 22:28:57 minikube kubelet[2573]: I0220 22:28:57.479074    2573 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ntb6v\" (UniqueName: \"kubernetes.io/projected/56ea022f-06c4-4c75-95f4-129e376ab73c-kube-api-access-ntb6v\") pod \"coredns-668d6bf9bc-74mbd\" (UID: \"56ea022f-06c4-4c75-95f4-129e376ab73c\") " pod="kube-system/coredns-668d6bf9bc-74mbd"
Feb 20 22:28:57 minikube kubelet[2573]: I0220 22:28:57.726487    2573 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="542963be4f558cd107108376faf5dc5563b7a5fe401eee1d1fb27219b34dbadc"
Feb 20 22:28:58 minikube kubelet[2573]: I0220 22:28:58.879259    2573 scope.go:117] "RemoveContainer" containerID="6a1bc09ebaa065f4348742a3bdaa4145391e65ccc360eb8ffe3f80cac3e723dd"
Feb 20 22:28:58 minikube kubelet[2573]: I0220 22:28:58.981469    2573 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-xnngs" podStartSLOduration=1.98107832 podStartE2EDuration="1.98107832s" podCreationTimestamp="2025-02-20 22:28:57 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-20 22:28:58.804730722 +0000 UTC m=+7.064826924" watchObservedRunningTime="2025-02-20 22:28:58.98107832 +0000 UTC m=+7.241174516"
Feb 20 22:28:59 minikube kubelet[2573]: I0220 22:28:59.926810    2573 scope.go:117] "RemoveContainer" containerID="6a1bc09ebaa065f4348742a3bdaa4145391e65ccc360eb8ffe3f80cac3e723dd"
Feb 20 22:28:59 minikube kubelet[2573]: I0220 22:28:59.927641    2573 scope.go:117] "RemoveContainer" containerID="037a37e61c6481e1f368f362be27d0b87eaa994df71fcaaaf2795ca441cb62ee"
Feb 20 22:28:59 minikube kubelet[2573]: E0220 22:28:59.927864    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(1c67a9b7-e35b-42d9-889f-b761b82530ae)\"" pod="kube-system/storage-provisioner" podUID="1c67a9b7-e35b-42d9-889f-b761b82530ae"
Feb 20 22:28:59 minikube kubelet[2573]: I0220 22:28:59.943628    2573 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-74mbd" podStartSLOduration=2.94350736 podStartE2EDuration="2.94350736s" podCreationTimestamp="2025-02-20 22:28:57 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-20 22:28:59.93441592 +0000 UTC m=+8.194512117" watchObservedRunningTime="2025-02-20 22:28:59.94350736 +0000 UTC m=+8.203603556"
Feb 20 22:29:00 minikube kubelet[2573]: I0220 22:29:00.946994    2573 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 20 22:29:00 minikube kubelet[2573]: I0220 22:29:00.947362    2573 scope.go:117] "RemoveContainer" containerID="037a37e61c6481e1f368f362be27d0b87eaa994df71fcaaaf2795ca441cb62ee"
Feb 20 22:29:00 minikube kubelet[2573]: E0220 22:29:00.947689    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(1c67a9b7-e35b-42d9-889f-b761b82530ae)\"" pod="kube-system/storage-provisioner" podUID="1c67a9b7-e35b-42d9-889f-b761b82530ae"
Feb 20 22:29:02 minikube kubelet[2573]: I0220 22:29:02.834978    2573 kuberuntime_manager.go:1702] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Feb 20 22:29:02 minikube kubelet[2573]: I0220 22:29:02.839474    2573 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Feb 20 22:29:03 minikube kubelet[2573]: I0220 22:29:03.131256    2573 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 20 22:29:12 minikube kubelet[2573]: I0220 22:29:12.086771    2573 scope.go:117] "RemoveContainer" containerID="037a37e61c6481e1f368f362be27d0b87eaa994df71fcaaaf2795ca441cb62ee"
Feb 20 22:29:13 minikube kubelet[2573]: I0220 22:29:13.120216    2573 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=19.120177109 podStartE2EDuration="19.120177109s" podCreationTimestamp="2025-02-20 22:28:54 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-20 22:29:13.120006286 +0000 UTC m=+21.380466023" watchObservedRunningTime="2025-02-20 22:29:13.120177109 +0000 UTC m=+21.380636848"
Feb 20 22:29:13 minikube kubelet[2573]: I0220 22:29:13.710639    2573 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lngw4\" (UniqueName: \"kubernetes.io/projected/8e312099-ae07-4d95-9375-0aa081c35f03-kube-api-access-lngw4\") pod \"vue-app-deployment-5f896bf957-d58jm\" (UID: \"8e312099-ae07-4d95-9375-0aa081c35f03\") " pod="default/vue-app-deployment-5f896bf957-d58jm"
Feb 20 22:29:14 minikube kubelet[2573]: I0220 22:29:14.530086    2573 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ba20c909c2ff84bde69a602aeea3c5357e28b52c353f92d32b4b28b2b3c89d85"
Feb 20 22:29:16 minikube kubelet[2573]: E0220 22:29:16.140743    2573 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="clippy:latest"
Feb 20 22:29:16 minikube kubelet[2573]: E0220 22:29:16.140862    2573 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="clippy:latest"
Feb 20 22:29:16 minikube kubelet[2573]: E0220 22:29:16.141091    2573 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:vue-container,Image:clippy:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lngw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod vue-app-deployment-5f896bf957-d58jm_default(8e312099-ae07-4d95-9375-0aa081c35f03): ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Feb 20 22:29:16 minikube kubelet[2573]: E0220 22:29:16.142405    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ErrImagePull: \"Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:29:16 minikube kubelet[2573]: E0220 22:29:16.556908    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ImagePullBackOff: \"Back-off pulling image \\\"clippy:latest\\\": ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:29:33 minikube kubelet[2573]: E0220 22:29:33.463929    2573 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="clippy:latest"
Feb 20 22:29:33 minikube kubelet[2573]: E0220 22:29:33.464067    2573 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="clippy:latest"
Feb 20 22:29:33 minikube kubelet[2573]: E0220 22:29:33.464381    2573 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:vue-container,Image:clippy:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lngw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod vue-app-deployment-5f896bf957-d58jm_default(8e312099-ae07-4d95-9375-0aa081c35f03): ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Feb 20 22:29:33 minikube kubelet[2573]: E0220 22:29:33.465828    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ErrImagePull: \"Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:29:47 minikube kubelet[2573]: E0220 22:29:47.085776    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ImagePullBackOff: \"Back-off pulling image \\\"clippy:latest\\\": ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:30:02 minikube kubelet[2573]: E0220 22:30:02.511513    2573 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="clippy:latest"
Feb 20 22:30:02 minikube kubelet[2573]: E0220 22:30:02.511629    2573 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="clippy:latest"
Feb 20 22:30:02 minikube kubelet[2573]: E0220 22:30:02.511817    2573 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:vue-container,Image:clippy:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lngw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod vue-app-deployment-5f896bf957-d58jm_default(8e312099-ae07-4d95-9375-0aa081c35f03): ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Feb 20 22:30:02 minikube kubelet[2573]: E0220 22:30:02.513241    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ErrImagePull: \"Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:30:13 minikube kubelet[2573]: E0220 22:30:13.086152    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ImagePullBackOff: \"Back-off pulling image \\\"clippy:latest\\\": ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:30:25 minikube kubelet[2573]: E0220 22:30:25.085905    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ImagePullBackOff: \"Back-off pulling image \\\"clippy:latest\\\": ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:30:38 minikube kubelet[2573]: E0220 22:30:38.085636    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ImagePullBackOff: \"Back-off pulling image \\\"clippy:latest\\\": ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:30:54 minikube kubelet[2573]: E0220 22:30:54.545291    2573 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="clippy:latest"
Feb 20 22:30:54 minikube kubelet[2573]: E0220 22:30:54.545391    2573 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="clippy:latest"
Feb 20 22:30:54 minikube kubelet[2573]: E0220 22:30:54.545582    2573 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:vue-container,Image:clippy:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lngw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod vue-app-deployment-5f896bf957-d58jm_default(8e312099-ae07-4d95-9375-0aa081c35f03): ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Feb 20 22:30:54 minikube kubelet[2573]: E0220 22:30:54.546999    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ErrImagePull: \"Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:31:08 minikube kubelet[2573]: E0220 22:31:08.085402    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ImagePullBackOff: \"Back-off pulling image \\\"clippy:latest\\\": ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:31:19 minikube kubelet[2573]: E0220 22:31:19.083670    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ImagePullBackOff: \"Back-off pulling image \\\"clippy:latest\\\": ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:31:33 minikube kubelet[2573]: E0220 22:31:33.084031    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ImagePullBackOff: \"Back-off pulling image \\\"clippy:latest\\\": ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:31:46 minikube kubelet[2573]: E0220 22:31:46.083600    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ImagePullBackOff: \"Back-off pulling image \\\"clippy:latest\\\": ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"
Feb 20 22:31:58 minikube kubelet[2573]: E0220 22:31:58.084003    2573 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vue-container\" with ImagePullBackOff: \"Back-off pulling image \\\"clippy:latest\\\": ErrImagePull: Error response from daemon: pull access denied for clippy, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/vue-app-deployment-5f896bf957-d58jm" podUID="8e312099-ae07-4d95-9375-0aa081c35f03"


==> storage-provisioner [037a37e61c64] <==
I0220 22:28:59.393030       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0220 22:28:59.412346       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority


==> storage-provisioner [ae67b5ba3865] <==
I0220 22:29:12.318381       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0220 22:29:12.334906       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0220 22:29:12.335144       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0220 22:29:12.356524       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0220 22:29:12.356753       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"b927df02-2fc6-41e9-a569-bc0a08a1a4a1", APIVersion:"v1", ResourceVersion:"427", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_323eabba-1992-41d3-8454-44b768d0ed64 became leader
I0220 22:29:12.357537       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_323eabba-1992-41d3-8454-44b768d0ed64!
I0220 22:29:12.461672       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_323eabba-1992-41d3-8454-44b768d0ed64!

